{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seqモデル\n",
    "\n",
    "入力文 -> LSTM Encoder -> 最終状態の隠れ層とメモリセル -> Decoder -> 生成文\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "入力文をベクトル化 <br />\n",
    "LSTMの隠れ層とメモリセルの最終状態を利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Encoder\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            rnn_input_size,\n",
    "            rnn_hidden_size,\n",
    "    ):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_input_size = rnn_input_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0) # パディングのword_idを0に指定\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            rnn_input_size,\n",
    "            rnn_hidden_size,\n",
    "            num_layers=1, # LSTM_1(LSTM_2(...のようにLSTM を多段にすることができる  今回は一層のみ\n",
    "            batch_first=True # 入力の形式を指定 (batch_size, 系列長, 分散表現の次元) のようにbatch_sizeが最初に来るよう入力\n",
    "        )\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.rnn_hidden_size),\n",
    "                torch.zeros(1, batch_size, self.rnn_hidden_size))\n",
    "\n",
    "        \n",
    "    def forward(self, input_sentence_list):\n",
    "        \"\"\"\n",
    "        順伝搬\n",
    "        :param input_sentence_list: Sentenceのリスト\n",
    "        :return hidden_cell_tensors: LSTMの最終状態の(隠れ層, メモリセル)\n",
    "        \"\"\" \n",
    "        self.zero_grad()\n",
    "        # 系列長が長い順にソート (ミニバッチ処理の際にlstmに入力するために必要)\n",
    "        input_sentence_idx_list = [i for i in range(len(input_sentence_list))]\n",
    "        input_sentence_idx_list.sort(key=lambda x: len(input_sentence_list[x]), reverse=True)\n",
    "        input_sentence_list = list(numpy.array(input_sentence_list)[input_sentence_idx_list])        \n",
    "        length_list = [len(sentence) for sentence in input_sentence_list]\n",
    "\n",
    "        # ミニバッチ処理の際の系列長を揃えるためのパディング処理\n",
    "        pad_sentence_list = nn.utils.rnn.pad_sequence([torch.tensor(sentence.word_id_list) for sentence in input_sentence_list], batch_first=True)\n",
    "\n",
    "        sentence_embeds = self.embeddings(pad_sentence_list) # 各単語をベクトル化\n",
    "        sentence_embeds = nn.utils.rnn.pack_padded_sequence(sentence_embeds, length_list, batch_first=True)\n",
    "\n",
    "        lstm_outputs, hidden_cell_tensors = self.lstm(sentence_embeds, self.init_hidden(len(input_sentence_list))) \n",
    "        lstm_outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(lstm_outputs, batch_first=True) \n",
    "\n",
    "        # 元の順にソートしなおし\n",
    "        idx_list = [i for i in range(len(input_sentence_list))]\n",
    "        align_keys = sorted(idx_list, key=lambda x: input_sentence_idx_list[x])\n",
    "        hidden_tensors, cell_tensors = hidden_cell_tensors\n",
    "        hidden_tensors = hidden_tensors[:, align_keys]\n",
    "        cell_tensors = cell_tensors[:, align_keys]\n",
    "        hidden_cell_tensors = (hidden_tensors, cell_tensors)\n",
    "        lstm_outputs = lstm_outputs[align_keys]        \n",
    "        \n",
    "        return hidden_cell_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderの最終状態をDecoderの初期状態として生成\n",
    "#### 学習時\n",
    "<img src=\"figures/seq2seq_train.jpg\" width=\"520px\" align=\"left\"><br clear=\"all\" />\n",
    "<br />\n",
    "\n",
    "#### 生成時\n",
    "<img src=\"figures/seq2seq_gen.jpg\" width=\"520px\" align=\"left\"><br clear=\"all\" />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import PartOfSentence\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Decoder\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            rnn_input_size,\n",
    "            rnn_hidden_size,\n",
    "    ):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_input_size = rnn_input_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.output_size = vocab_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0) # パディングのword_idを0に指定\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            rnn_input_size,\n",
    "            rnn_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_linear = nn.Linear(rnn_hidden_size, vocab_size) # LSTMの各出力を語彙数の次元に変換する線形層\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, input_sentence_list, enc_hidden_cell):\n",
    "        \"\"\"\n",
    "        順伝搬\n",
    "        :param input_sentence_list: Sentenceのリスト\n",
    "        :return outputs: decoder LSTM各時点での出力\n",
    "        :return hidden_cell_tensors: LSTMの最終状態の(隠れ層, メモリセル) (生成時に必要)\n",
    "        \"\"\" \n",
    "        self.zero_grad()\n",
    "        length_list = [len(sentence) for sentence in input_sentence_list]\n",
    "        \n",
    "        # ミニバッチ処理の際の系列長を揃えるためのパディング処理\n",
    "        pad_sentence_list = nn.utils.rnn.pad_sequence([torch.tensor(sentence.word_id_list) for sentence in input_sentence_list], batch_first=True)\n",
    "        \n",
    "        sentence_embeds = self.embeddings(pad_sentence_list) # 各単語をベクトル化\n",
    "        sentence_embeds = nn.utils.rnn.pack_padded_sequence(sentence_embeds, length_list, batch_first=True)\n",
    "        lstm_outputs, hidden_cell_tensors = self.lstm(sentence_embeds, enc_hidden_cell)\n",
    "        lstm_outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(lstm_outputs, batch_first=True)\n",
    "        # クラス数次元に変換\n",
    "        outputs = self.output_linear(lstm_outputs)\n",
    "\n",
    "        return outputs, hidden_cell_tensors\n",
    "\n",
    "\n",
    "    def forward_labels_loss(self, input_sentence_list, enc_hidden_cell):\n",
    "        \"\"\"\n",
    "        forward()による順伝搬とloss計算\n",
    "        :params input_sentence_list: Sentenceのリスト\n",
    "        :return loss: 損失\n",
    "        :return predicted_label_ids : 予測ラベルのid\n",
    "        :return softmax_scores: 各予測の確率\n",
    "        \"\"\"\n",
    "        # 系列長が長い順にソート (ミニバッチ処理の際にlstmに入力するために必要)\n",
    "        input_sentence_idx_list = [i for i in range(len(input_sentence_list))]\n",
    "        input_sentence_idx_list.sort(key=lambda x: len(input_sentence_list[x]), reverse=True)\n",
    "        sorted_input_sentence_list = list(numpy.array(input_sentence_list)[input_sentence_idx_list])\n",
    "\n",
    "        # encからの結果も合わせてソート\n",
    "        hidden_tensors, cell_tensors = enc_hidden_cell\n",
    "        hidden_tensors = hidden_tensors[:, input_sentence_idx_list]\n",
    "        cell_tensors = cell_tensors[:, input_sentence_idx_list]\n",
    "        enc_hidden_cell = (hidden_tensors, cell_tensors)\n",
    "\n",
    "        # 順伝搬\n",
    "        outputs, _ = self.forward(sorted_input_sentence_list, enc_hidden_cell)\n",
    "        outputs = outputs[:, :-1, :] # <eos>入力時の出力だけ除外\n",
    "\n",
    "        # 損失計算\n",
    "        loss = 0\n",
    "        for sent, output in zip(sorted_input_sentence_list, outputs):\n",
    "            true_seq = torch.tensor(sent.word_id_list)[1:]\n",
    "            output = output[:len(true_seq)]\n",
    "            loss += self.loss_function(output, true_seq)\n",
    "\n",
    "        # 元の順にソートしなおし\n",
    "        idx_list = [i for i in range(len(input_sentence_list))]\n",
    "        align_keys = sorted(idx_list, key=lambda x: input_sentence_idx_list[x])\n",
    "        outputs = outputs[align_keys]\n",
    "            \n",
    "        # 確率値と予測ラベル\n",
    "        softmax_scores = nn.functional.softmax(outputs, dim=2)\n",
    "        predicted_label_ids = softmax_scores.argmax(2)\n",
    "\n",
    "        return loss, predicted_label_ids, softmax_scores\n",
    "\n",
    "    \n",
    "    def generate(self, start_id, enc_hidden_cell, end_tag, max_generate_length):\n",
    "        \"\"\"\n",
    "        生成（一単語ずつforward）\n",
    "        :param start_id: 生成のために最初にデコーダに入力する<sos>の単語id\n",
    "        :param enc_hidden_cell: エンコーダの最終状態の(h, c)\n",
    "        :param end_tag: 生成を終了するための<eos>の単語id\n",
    "        :param max_generate_length: 生成の最大長 (<eos>が出力されなくても生成を終了)\n",
    "        :return predicted_label_id_list: 生成された単語idリスト\n",
    "        \"\"\"\n",
    "        h = enc_hidden_cell[0]\n",
    "        c = enc_hidden_cell[1]\n",
    "        predicted_label_id_list = []\n",
    "        word_id = start_id\n",
    "        for _ in range(max_generate_length):\n",
    "            outputs, (h, c) = self.forward([word_id], (h, c)) # 最初は<sos>を読み込ませて，あとは一単語ずつ生成\n",
    "            softmax_scores = nn.functional.softmax(outputs, dim=2)\n",
    "            predicted_label_id = softmax_scores.argmax(2)\n",
    "            predicted_label_id_list.append(predicted_label_id[0].item())\n",
    "            if predicted_label_id[0].item() == end_tag:\n",
    "                break\n",
    "            word_id = PartOfSentence(predicted_label_id[0].item()) # forwardに入力するための形式に (自作)\n",
    "        return predicted_label_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq (Encoder+Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import PartOfSentence\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoderモデル\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 rnn_input_size,\n",
    "                 rnn_hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = LSTMEncoder(vocab_size, embedding_size, rnn_input_size, rnn_hidden_size)\n",
    "        self.decoder = LSTMDecoder(vocab_size, embedding_size, rnn_input_size, rnn_hidden_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input_sentence_list):\n",
    "        \"\"\"\n",
    "        学習の際の順伝搬とロス計算\n",
    "        :param input_sentence_list: Sentenceのリスト\n",
    "        :return loss: デコーダの出力から計算される損失\n",
    "        :return pred_ids: 予測単語のid\n",
    "        :return scores: 各予測のスコア\n",
    "        \"\"\"\n",
    "        enc_input_sentence_list, dec_input_sentence_list = [], []\n",
    "        for sent1, sent2 in input_sentence_list:\n",
    "            enc_input_sentence_list.append(sent1)\n",
    "            dec_input_sentence_list.append(sent2)\n",
    "        # エンコーダの順伝搬\n",
    "        hidden_cell_tensors = self.encoder.forward(enc_input_sentence_list)\n",
    "        # デコーダの順伝搬とロス計算\n",
    "        loss, pred_ids, scores = self.decoder.forward_labels_loss(dec_input_sentence_list, hidden_cell_tensors)\n",
    "        return loss, pred_ids, scores\n",
    "\n",
    "    \n",
    "    def generate(self, input_sentence_list, start_tag, end_tag):\n",
    "        \"\"\"\n",
    "        生成の際の順伝搬\n",
    "        :param input_sentence_list: Sentenceのリスト\n",
    "        :param start_tag: <sos>タグのタグid\n",
    "        :param eos_tag: <eos>タグのタグid\n",
    "        \"\"\"\n",
    "        pred_ids_list = []\n",
    "        enc_input_sentence_list = [sent for sent, _ in input_sentence_list]\n",
    "        for enc_input_sentence in enc_input_sentence_list: # 一文ずつ処理\n",
    "            # 入力文のエンコード\n",
    "            hidden_cell_tensors = self.encoder.forward([enc_input_sentence])\n",
    "            # デコーダによる生成\n",
    "            pred_ids = self.decoder.generate(PartOfSentence(start_tag), hidden_cell_tensors, end_tag, 20)\n",
    "            pred_ids_list.append(pred_ids)\n",
    "        return pred_ids_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.8267, grad_fn=<AddBackward0>)\n",
      "tensor(21.8759, grad_fn=<AddBackward0>)\n",
      "tensor(10.1230, grad_fn=<AddBackward0>)\n",
      "tensor(3.7304, grad_fn=<AddBackward0>)\n",
      "tensor(2.2909, grad_fn=<AddBackward0>)\n",
      "tensor(1.7811, grad_fn=<AddBackward0>)\n",
      "tensor(1.3168, grad_fn=<AddBackward0>)\n",
      "tensor(0.8391, grad_fn=<AddBackward0>)\n",
      "tensor(0.5525, grad_fn=<AddBackward0>)\n",
      "tensor(0.3453, grad_fn=<AddBackward0>)\n",
      "tensor(0.2708, grad_fn=<AddBackward0>)\n",
      "tensor(0.2169, grad_fn=<AddBackward0>)\n",
      "tensor(0.1504, grad_fn=<AddBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "tensor(0.0689, grad_fn=<AddBackward0>)\n",
      "tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0335, grad_fn=<AddBackward0>)\n",
      "tensor(0.0260, grad_fn=<AddBackward0>)\n",
      "tensor(0.0206, grad_fn=<AddBackward0>)\n",
      "tensor(0.0165, grad_fn=<AddBackward0>)\n",
      "tensor(0.0134, grad_fn=<AddBackward0>)\n",
      "tensor(0.0108, grad_fn=<AddBackward0>)\n",
      "tensor(0.0087, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "tensor(0.0050, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "from data import Seq2SeqCorpus # 自作クラス (data.py参照)\n",
    "\n",
    "def trainer(model, corpus):\n",
    "    model.train()\n",
    "    op = optim.Adam(model.parameters(), lr=0.1)\n",
    "    for epoch in range(100):\n",
    "        loss, pred_ids, scores = model.forward(corpus.train_sentence_pair_list)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "corpus = Seq2SeqCorpus(\"text_pairs.txt\", test_file=\"text_pairs.txt\")\n",
    "seq2seq = Seq2Seq(len(corpus.word_to_id_dict), 100, 100, 100)\n",
    "\n",
    "seq2seq.train()\n",
    "op = optim.Adam(seq2seq.parameters(), lr=0.1)\n",
    "for epoch in range(50):\n",
    "    loss, pred_ids, scores = seq2seq.forward(corpus.train_sentence_pair_list)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    op.step()\n",
    "    seq2seq.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力文： おはようございます。\n",
      "正解文： おはよう！\n",
      "予測文： おはよう！<eos>\n",
      "\n",
      "入力文： こんにちは。\n",
      "正解文： こんにちは。よろしくお願いします。\n",
      "予測文： こんにちは。よろしくお願いします。<eos>\n",
      "\n",
      "入力文： 普段何をしていますか？\n",
      "正解文： 学校で働いています。\n",
      "予測文： 学校で働いています。<eos>\n",
      "\n",
      "入力文： どこに住んでいますか？\n",
      "正解文： 東京に住んでいます。\n",
      "予測文： 東京に住んでいます。<eos>\n",
      "\n",
      "入力文： どこかに旅行に行こう\n",
      "正解文： 北海道に行きたいです。\n",
      "予測文： 北海道に行きたいです。<eos>\n",
      "\n",
      "入力文： おいくつですか。\n",
      "正解文： 二十歳です。\n",
      "予測文： 二十歳です。<eos>\n",
      "\n",
      "入力文： どんなジャンルの音楽が好きですか？\n",
      "正解文： ジャズが好きです。\n",
      "予測文： ジャズが好きです。<eos>\n",
      "\n",
      "入力文： 泳ぐのは好きですか。\n",
      "正解文： はい。\n",
      "予測文： はい。<eos>\n",
      "\n",
      "入力文： 誕生日はいつですか。\n",
      "正解文： 11月です。\n",
      "予測文： 11月です。<eos>\n",
      "\n",
      "入力文： お会いできてよかったです。\n",
      "正解文： こちらこそ。\n",
      "予測文： こちらこそ。<eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq2seq.eval()\n",
    "start_tag_id = corpus.word_to_id_dict[\"<sos>\"]\n",
    "end_tag_id = corpus.word_to_id_dict[\"<eos>\"]\n",
    "pred_ids = seq2seq.generate(corpus.test_sentence_pair_list, start_tag_id, end_tag_id)\n",
    "\n",
    "for (tr_sent1, tr_sent2), (sent, _), pred in zip(corpus.train_sentence_pair_list, corpus.test_sentence_pair_list, pred_ids):\n",
    "    print(\"入力文：\", tr_sent1.text)\n",
    "    print(\"正解文：\", tr_sent2.text)\n",
    "    print(\"予測文：\", \"\".join([corpus.id_to_word_dict[w_id] for w_id in pred]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
