{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from multi_layer import MultiLayer # 多層ニューラルモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインスタンス\n",
    "multi_layer_model = MultiLayer()\n",
    "\n",
    "# 損失の最小化のための最適化手法\n",
    "op = optim.SGD(multi_layer_model.parameters(), lr=0.1) # lr:learning rate (学習率)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力と正解値 (50データ)\n",
    "input_vector = torch.rand(50, 1)\n",
    "target_values = torch.reshape(torch.tensor([[i[0]+1 for i in input_vector]]), (50, 1))\n",
    "train_data = torch.utils.data.TensorDataset(input_vector, target_values)\n",
    "train_data_size = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチサイズ (10データずつ)\n",
    "minibatch_size = 10\n",
    "# イテレーション数 (全データ数/ミニバッチサイズ)\n",
    "max_batch_no = train_data_size // minibatch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1エポック (今回は5イテレーションで全データ1周)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no: 1/5 loss: 0.4344821572303772\n",
      "batch_no: 2/5 loss: 0.551845133304596\n",
      "batch_no: 3/5 loss: 0.2868766486644745\n",
      "batch_no: 4/5 loss: 0.37962377071380615\n",
      "batch_no: 5/5 loss: 0.4316334128379822\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチ学習のためのデータローダー\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "# ミニバッチごとに逆伝搬とパラメータ更新\n",
    "# イテレーション\n",
    "for batch_no, (batch_input_vector, batch_target_values) in enumerate(train_data_loader):            \n",
    "    # 順伝搬と損失計算\n",
    "    loss = multi_layer_model.forward_loss(batch_input_vector, batch_target_values)\n",
    "    print(\"batch_no: {}/{} loss: {}\".format(batch_no+1, max_batch_no, loss))\n",
    "\n",
    "    # 逆伝搬 (勾配の設定)\n",
    "    loss.backward()\n",
    "\n",
    "    # パラメータの更新\n",
    "    op.step()\n",
    "\n",
    "    # 勾配の消去\n",
    "    multi_layer_model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エポック単位の繰り返しによる学習 (全データ3周)\n",
    "全データではなく一部をサンプリングして学習することを繰り返すとき，<br />\n",
    "最適化は，最急勾配法ではなく，<b>確率的勾配降下法（stochastic gradient descent, SGD）</b>と呼ばれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "batch_no: 1/5 loss: 0.2325318306684494\n",
      "batch_no: 2/5 loss: 0.2521873414516449\n",
      "batch_no: 3/5 loss: 0.38480204343795776\n",
      "batch_no: 4/5 loss: 0.614579975605011\n",
      "batch_no: 5/5 loss: 0.5559219121932983\n",
      "==========\n",
      "epoch:  1\n",
      "batch_no: 1/5 loss: 0.3187640309333801\n",
      "batch_no: 2/5 loss: 0.3790613114833832\n",
      "batch_no: 3/5 loss: 0.4279140830039978\n",
      "batch_no: 4/5 loss: 0.42195239663124084\n",
      "batch_no: 5/5 loss: 0.4608399271965027\n",
      "==========\n",
      "epoch:  2\n",
      "batch_no: 1/5 loss: 0.41695159673690796\n",
      "batch_no: 2/5 loss: 0.35896867513656616\n",
      "batch_no: 3/5 loss: 0.35146814584732056\n",
      "batch_no: 4/5 loss: 0.5163618326187134\n",
      "batch_no: 5/5 loss: 0.341625839471817\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"epoch: \", epoch)\n",
    "\n",
    "    # ミニバッチ学習のためのデータローダー\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    # ミニバッチごとに逆伝搬とパラメータ更新\n",
    "    # イテレーション\n",
    "    for batch_no, (batch_input_vector, batch_target_values) in enumerate(train_data_loader):            \n",
    "        # 順伝搬と損失計算\n",
    "        loss = multi_layer_model.forward_loss(batch_input_vector, batch_target_values)\n",
    "        print(\"batch_no: {}/{} loss: {}\".format(batch_no+1, max_batch_no, loss))\n",
    "\n",
    "        # 逆伝搬 (勾配の設定)\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        op.step()\n",
    "\n",
    "        # 勾配の消去\n",
    "        multi_layer_model.zero_grad()\n",
    "\n",
    "    print(\"=\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
