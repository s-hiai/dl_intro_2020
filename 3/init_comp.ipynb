{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重み初期化の比較\n",
    "ランダム (一様分布による初期化)とxavierによる初期化を比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    単層モデルクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, init_val, xavier=False):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = torch.nn.Linear(10, 1) # wx+b 初期パラメータはランダム\n",
    "        self.linear.weight = torch.nn.Parameter(torch.reshape(torch.tensor(init_val), (1, 10)))\n",
    "        if xavier: # フラグでxaviarありなしを設定\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        # 損失関数\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "        \n",
    "    def forward(self, input_vector):\n",
    "        \"\"\"\n",
    "        順伝搬\n",
    "        :param input_vector: 入力ベクトル\n",
    "        :return tanh_w3x: モデルの出力\n",
    "        \"\"\"\n",
    "        wx = self.linear(input_vector)\n",
    "        return wx\n",
    "\n",
    "    \n",
    "    def forward_loss(self, input_vector, target_values):\n",
    "        \"\"\"\n",
    "        順伝搬 + 損失計算\n",
    "        :param input_vector: 入力ベクトル\n",
    "        :param target_values: 正解の値\n",
    "        :return loss:損失\n",
    "        \"\"\"\n",
    "        wx = self.forward(input_vector)\n",
    "        loss = self.loss_func(wx, target_values)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "xavierと一様分布でそれぞれ初期化したモデルで同様に学習をして損失を比較<br />\n",
    "複数回学習を実行して最終的な損失の平均を比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, minibatch_size, linear_model_xavier, linear_model):\n",
    "    # 損失の最小化のための最適化手法\n",
    "    op_xavier = optim.SGD(linear_model_xavier.parameters(), lr=0.1) # lr:learning rate (学習率)\n",
    "    op = optim.SGD(linear_model.parameters(), lr=0.1) # lr:learning rate (学習率)\n",
    "\n",
    "    train_data_size = len(train_data)\n",
    "    max_batch_no = train_data_size // minibatch_size\n",
    "    # 学習\n",
    "    # エポック\n",
    "    for epoch in range(100):\n",
    "        #print(\"epoch: \", epoch)\n",
    "\n",
    "        # ミニバッチ学習のためのデータローダー\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True)\n",
    "        \n",
    "        # ミニバッチごとに逆伝搬とパラメータ更新\n",
    "        # イテレーション\n",
    "        for batch_no, (batch_input_vector, batch_target_values) in enumerate(train_data_loader):            \n",
    "            # 順伝搬と損失計算\n",
    "            loss_xavier = linear_model_xavier.forward_loss(batch_input_vector, batch_target_values)\n",
    "            loss = linear_model.forward_loss(batch_input_vector, batch_target_values)\n",
    "            #print(\"batch_no: {}/{} loss_xavier: {}, loss: {}\".format(batch_no+1, max_batch_no, loss_xavier, los))\n",
    "\n",
    "            # 逆伝搬 (勾配の設定)\n",
    "            loss_xavier.backward()\n",
    "            loss.backward()\n",
    "\n",
    "            # パラメータの更新\n",
    "            op_xavier.step()\n",
    "            op.step()\n",
    "\n",
    "            # 勾配の消去\n",
    "            linear_model_xavier.zero_grad()\n",
    "            linear_model.zero_grad()\n",
    "\n",
    "        #print(\"=\"*10)\n",
    "    \n",
    "    return loss_xavier, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0: loss_xavier 1.669778794166632e-05, loss 8.861310925567523e-05\n",
      "test 1: loss_xavier 4.0351494590140646e-08, loss 5.493638468578865e-07\n",
      "test 2: loss_xavier 1.7073205349493747e-10, loss 2.4228157258221472e-09\n",
      "test 3: loss_xavier 1.7621459304749398e-13, loss 2.3291590010854346e-12\n",
      "test 4: loss_xavier 9.5212723881348e-14, loss 1.4637180627708607e-13\n",
      "test 5: loss_xavier 8.668621240747951e-14, loss 8.526512829121202e-14\n",
      "test 6: loss_xavier 1.3500311979441904e-13, loss 1.2789769243681803e-13\n",
      "test 7: loss_xavier 6.252776210214153e-14, loss 4.831690738693953e-14\n",
      "test 8: loss_xavier 1.0089706712268151e-13, loss 9.805489889014654e-14\n",
      "test 9: loss_xavier 4.6895819882540254e-14, loss 4.6895819882540254e-14\n",
      "test 10: loss_xavier 6.252776210214153e-14, loss 7.531753270107605e-14\n",
      "test 11: loss_xavier 5.4001247240141256e-14, loss 5.684341886080802e-14\n",
      "test 12: loss_xavier 4.6895819882540254e-14, loss 5.684341886080802e-14\n",
      "test 13: loss_xavier 1.7053026335868762e-14, loss 1.9895196262469626e-14\n",
      "test 14: loss_xavier 4.831690738693953e-14, loss 3.6948220904272494e-14\n",
      "test 15: loss_xavier 4.831690738693953e-14, loss 4.4053648261873496e-14\n",
      "test 16: loss_xavier 4.263256414560601e-14, loss 5.11590756194745e-14\n",
      "test 17: loss_xavier 7.105427357601002e-14, loss 5.968558709334298e-14\n",
      "test 18: loss_xavier 3.4106052671737525e-14, loss 3.552713678800501e-14\n",
      "test 19: loss_xavier 3.979039252493925e-14, loss 4.4053648261873496e-14\n",
      "test 20: loss_xavier 8.526512829121202e-14, loss 4.547473576627277e-14\n",
      "test 21: loss_xavier 8.668621240747951e-14, loss 8.526512829121202e-14\n",
      "test 22: loss_xavier 6.821210534347505e-14, loss 8.242296005867705e-14\n",
      "test 23: loss_xavier 7.389644180854499e-14, loss 8.526512829121202e-14\n",
      "test 24: loss_xavier 3.4106052671737525e-14, loss 3.4106052671737525e-14\n",
      "test 25: loss_xavier 3.1263881051070766e-14, loss 2.4158453693469764e-14\n",
      "test 26: loss_xavier 6.53699303346765e-14, loss 7.815970093361102e-14\n",
      "test 27: loss_xavier 1.1368683941568192e-14, loss 7.105427357601002e-15\n",
      "test 28: loss_xavier 6.394884621840902e-14, loss 5.82645029770755e-14\n",
      "test 29: loss_xavier 6.252776210214153e-14, loss 6.394884621840902e-14\n"
     ]
    }
   ],
   "source": [
    "init_val = [random.random() for _ in range(10)]\n",
    "\n",
    "# モデル\n",
    "linear_model_xavier = LinearLayer(init_val, xavier=True)\n",
    "linear_model = LinearLayer(init_val)\n",
    "\n",
    "# 入力とラベル\n",
    "input_vector = torch.rand(50, 10)\n",
    "target_values = torch.reshape(torch.tensor([[i[0]+1 for i in input_vector]]), (50, 1))\n",
    "train_data = torch.utils.data.TensorDataset(input_vector, target_values)\n",
    "\n",
    "# ミニバッチサイズ\n",
    "minibatch_size = 10\n",
    "\n",
    "# 30回試行して後で平均をとる\n",
    "loss_xavier_list = []\n",
    "loss_list = []\n",
    "for i in range(30):\n",
    "    loss_xavier, loss = train(train_data, minibatch_size, linear_model_xavier, linear_model)\n",
    "    loss_xavier_list.append(float(loss_xavier))\n",
    "    loss_list.append(float(loss))\n",
    "    print(\"test {}: loss_xavier {}, loss {}\".format(i, float(loss_xavier), float(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_xavier_ave: 5.579437306362417e-07, loss_ave: 2.9721633281572666e-06\n"
     ]
    }
   ],
   "source": [
    "# 平均を出力\n",
    "print(\"loss_xavier_ave: {}, loss_ave: {}\".format(sum(loss_xavier_list)/len(loss_xavier_list), \n",
    "                                                 sum(loss_list)/len(loss_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
