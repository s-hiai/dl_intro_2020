{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropoutありのモデルの定義\n",
    "nn.Dropoutを利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class MultiLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    多層モデルクラス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MultiLayer, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(10, 1) # wx+b 初期パラメータはランダム\n",
    "\n",
    "        # dropoutの追加\n",
    "        self.dropout = nn.Dropout(p=0.5) # p:dropout対象のパラメータの割合\n",
    "        \n",
    "        # 損失関数\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "        \n",
    "    def forward(self, input_vector):\n",
    "        \"\"\"\n",
    "        順伝搬\n",
    "        :param input_vector: 入力ベクトル\n",
    "        :return tanh_w3x: モデルの出力\n",
    "        \"\"\"\n",
    "        w1x = self.linear1(input_vector)\n",
    "        print(\"dropout前: \",w1x)\n",
    "        w1x = self.dropout(w1x)\n",
    "        print(\"dropout後: \",w1x)\n",
    "        return w1x\n",
    "\n",
    "    \n",
    "    def forward_loss(self, input_vector, target_values):\n",
    "        \"\"\"\n",
    "        順伝搬 + 損失計算\n",
    "        :param input_vector: 入力ベクトル\n",
    "        :param target_values: 正解の値\n",
    "        :return loss:損失\n",
    "        \"\"\"\n",
    "        wx = self.forward(input_vector)\n",
    "        loss = self.loss_func(wx, target_values)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインスタンス\n",
    "multi_layer_model = MultiLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "dropout前:  tensor([[0.5999],\n",
      "        [0.8199],\n",
      "        [0.8084],\n",
      "        [0.6593],\n",
      "        [0.7123],\n",
      "        [0.6373],\n",
      "        [0.5845],\n",
      "        [0.4164],\n",
      "        [0.6212],\n",
      "        [0.6483]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [1.6168],\n",
      "        [0.0000],\n",
      "        [1.4247],\n",
      "        [0.0000],\n",
      "        [1.1690],\n",
      "        [0.8327],\n",
      "        [1.2423],\n",
      "        [1.2967]], grad_fn=<MulBackward0>)\n",
      "batch_no: 1/5 loss: 1.0170060396194458\n",
      "dropout前:  tensor([[0.5016],\n",
      "        [0.8467],\n",
      "        [0.7798],\n",
      "        [1.0743],\n",
      "        [0.9552],\n",
      "        [0.6635],\n",
      "        [0.9582],\n",
      "        [0.6690],\n",
      "        [1.0536],\n",
      "        [1.0290]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.0032],\n",
      "        [1.6934],\n",
      "        [1.5595],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.3270],\n",
      "        [0.0000],\n",
      "        [1.3380],\n",
      "        [2.1072],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 2/5 loss: 1.2145252227783203\n",
      "dropout前:  tensor([[1.0138],\n",
      "        [0.7964],\n",
      "        [0.7510],\n",
      "        [0.6641],\n",
      "        [0.9672],\n",
      "        [0.7774],\n",
      "        [0.9433],\n",
      "        [0.9475],\n",
      "        [0.8042],\n",
      "        [0.8062]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [1.5019],\n",
      "        [1.3281],\n",
      "        [0.0000],\n",
      "        [1.5548],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.6085],\n",
      "        [1.6123]], grad_fn=<MulBackward0>)\n",
      "batch_no: 3/5 loss: 1.6320180892944336\n",
      "dropout前:  tensor([[0.8692],\n",
      "        [0.6887],\n",
      "        [0.9760],\n",
      "        [0.7546],\n",
      "        [0.7218],\n",
      "        [0.6635],\n",
      "        [0.7541],\n",
      "        [0.8255],\n",
      "        [0.8878],\n",
      "        [0.9183]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.7385],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.4436],\n",
      "        [0.0000],\n",
      "        [1.5082],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 4/5 loss: 1.3089627027511597\n",
      "dropout前:  tensor([[0.9833],\n",
      "        [1.0049],\n",
      "        [1.0446],\n",
      "        [0.5118],\n",
      "        [0.9148],\n",
      "        [0.9619],\n",
      "        [0.8350],\n",
      "        [0.8605],\n",
      "        [0.7913],\n",
      "        [0.7061]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [2.0097],\n",
      "        [2.0893],\n",
      "        [1.0236],\n",
      "        [0.0000],\n",
      "        [1.9238],\n",
      "        [1.6700],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.4123]], grad_fn=<MulBackward0>)\n",
      "batch_no: 5/5 loss: 1.1718436479568481\n",
      "==========\n",
      "epoch:  1\n",
      "dropout前:  tensor([[0.6946],\n",
      "        [0.5409],\n",
      "        [0.7975],\n",
      "        [0.4293],\n",
      "        [0.6431],\n",
      "        [0.6989],\n",
      "        [0.7340],\n",
      "        [0.3872],\n",
      "        [0.7403],\n",
      "        [0.6019]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [1.5950],\n",
      "        [0.8586],\n",
      "        [1.2862],\n",
      "        [1.3978],\n",
      "        [1.4679],\n",
      "        [0.0000],\n",
      "        [1.4806],\n",
      "        [1.2037]], grad_fn=<MulBackward0>)\n",
      "batch_no: 1/5 loss: 0.8576442003250122\n",
      "dropout前:  tensor([[0.9305],\n",
      "        [0.8885],\n",
      "        [0.9365],\n",
      "        [0.5659],\n",
      "        [0.9459],\n",
      "        [0.9633],\n",
      "        [1.1088],\n",
      "        [0.8554],\n",
      "        [0.8230],\n",
      "        [0.7533]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.8610],\n",
      "        [1.7770],\n",
      "        [1.8730],\n",
      "        [1.1318],\n",
      "        [1.8918],\n",
      "        [0.0000],\n",
      "        [2.2176],\n",
      "        [0.0000],\n",
      "        [1.6461],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 2/5 loss: 0.7080027461051941\n",
      "dropout前:  tensor([[0.7360],\n",
      "        [0.8664],\n",
      "        [0.6882],\n",
      "        [0.6273],\n",
      "        [0.7603],\n",
      "        [0.6658],\n",
      "        [0.9407],\n",
      "        [0.9203],\n",
      "        [0.6912],\n",
      "        [0.8870]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [1.7327],\n",
      "        [0.0000],\n",
      "        [1.2546],\n",
      "        [1.5206],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.8406],\n",
      "        [1.3824],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 3/5 loss: 1.3110792636871338\n",
      "dropout前:  tensor([[0.8018],\n",
      "        [0.7281],\n",
      "        [0.7487],\n",
      "        [0.8498],\n",
      "        [0.8479],\n",
      "        [0.5590],\n",
      "        [0.5875],\n",
      "        [0.6479],\n",
      "        [0.7874],\n",
      "        [0.6115]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.6036],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.6997],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.1751],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 4/5 loss: 1.286100149154663\n",
      "dropout前:  tensor([[0.8144],\n",
      "        [0.8931],\n",
      "        [0.5201],\n",
      "        [0.8619],\n",
      "        [0.9653],\n",
      "        [0.6943],\n",
      "        [0.8580],\n",
      "        [0.7990],\n",
      "        [0.9371],\n",
      "        [0.8088]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.6287],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.9305],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.5980],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 5/5 loss: 1.806270956993103\n",
      "==========\n",
      "epoch:  2\n",
      "dropout前:  tensor([[0.7272],\n",
      "        [0.7351],\n",
      "        [0.7513],\n",
      "        [0.8013],\n",
      "        [0.7508],\n",
      "        [0.7737],\n",
      "        [0.8535],\n",
      "        [0.7844],\n",
      "        [0.3950],\n",
      "        [0.6078]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.5017],\n",
      "        [0.0000],\n",
      "        [1.7070],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 1/5 loss: 2.265317440032959\n",
      "dropout前:  tensor([[0.7210],\n",
      "        [0.7408],\n",
      "        [0.6725],\n",
      "        [0.7032],\n",
      "        [0.6829],\n",
      "        [0.5415],\n",
      "        [0.5735],\n",
      "        [0.6688],\n",
      "        [0.6011],\n",
      "        [0.8075]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [1.4815],\n",
      "        [1.3450],\n",
      "        [1.4063],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.3376],\n",
      "        [1.2023],\n",
      "        [1.6150]], grad_fn=<MulBackward0>)\n",
      "batch_no: 2/5 loss: 0.9883270263671875\n",
      "dropout前:  tensor([[0.8068],\n",
      "        [0.6910],\n",
      "        [0.9121],\n",
      "        [0.6259],\n",
      "        [0.9635],\n",
      "        [0.6622],\n",
      "        [0.7954],\n",
      "        [0.8390],\n",
      "        [0.5408],\n",
      "        [0.7797]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.6137],\n",
      "        [1.3821],\n",
      "        [1.8243],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.6780],\n",
      "        [1.0816],\n",
      "        [1.5595]], grad_fn=<MulBackward0>)\n",
      "batch_no: 3/5 loss: 0.9259475469589233\n",
      "dropout前:  tensor([[0.8846],\n",
      "        [0.6139],\n",
      "        [0.7653],\n",
      "        [0.9084],\n",
      "        [0.5076],\n",
      "        [0.5982],\n",
      "        [0.7554],\n",
      "        [0.8060],\n",
      "        [0.4761],\n",
      "        [0.8740]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[1.7692],\n",
      "        [0.0000],\n",
      "        [1.5306],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.6119],\n",
      "        [0.0000],\n",
      "        [1.7479]], grad_fn=<MulBackward0>)\n",
      "batch_no: 4/5 loss: 1.1942955255508423\n",
      "dropout前:  tensor([[0.8754],\n",
      "        [0.8688],\n",
      "        [0.7257],\n",
      "        [0.5814],\n",
      "        [0.7028],\n",
      "        [0.5973],\n",
      "        [0.6355],\n",
      "        [0.5774],\n",
      "        [0.5699],\n",
      "        [0.6906]], grad_fn=<AddmmBackward>)\n",
      "dropout後:  tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.1947],\n",
      "        [1.2711],\n",
      "        [0.0000],\n",
      "        [1.1398],\n",
      "        [0.0000]], grad_fn=<MulBackward0>)\n",
      "batch_no: 5/5 loss: 1.8623062372207642\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# 損失の最小化のための最適化手法\n",
    "op = optim.SGD(multi_layer_model.parameters(), lr=0.1) # lr:learning rate (学習率)\n",
    "\n",
    "# 入力とラベル\n",
    "input_vector = torch.rand(50, 10)\n",
    "target_values = torch.reshape(torch.tensor([[i[0]+1 for i in input_vector]]), (50, 1))\n",
    "train_data = torch.utils.data.TensorDataset(input_vector, target_values)\n",
    "train_data_size = len(train_data)\n",
    "\n",
    "# ミニバッチサイズ\n",
    "minibatch_size = 10\n",
    "max_batch_no = train_data_size // minibatch_size\n",
    "\n",
    "# 学習\n",
    "# エポック\n",
    "for epoch in range(3):\n",
    "    print(\"epoch: \", epoch)\n",
    "\n",
    "    # ミニバッチ学習のためのデータローダー\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    # ミニバッチごとに逆伝搬とパラメータ更新\n",
    "    # イテレーション\n",
    "    for batch_no, (batch_input_vector, batch_target_values) in enumerate(train_data_loader):            \n",
    "        # 順伝搬と損失計算\n",
    "        loss = multi_layer_model.forward_loss(batch_input_vector, batch_target_values)\n",
    "        print(\"batch_no: {}/{} loss: {}\".format(batch_no+1, max_batch_no, loss))\n",
    "\n",
    "\n",
    "        # 逆伝搬 (勾配の設定)\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        op.step()\n",
    "\n",
    "        # 勾配の消去\n",
    "        multi_layer_model.zero_grad()\n",
    "\n",
    "\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
