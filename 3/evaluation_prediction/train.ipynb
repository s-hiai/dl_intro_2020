{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練・開発データを利用した学習\n",
    "訓練時に使っていないデータで評価することで過学習を抑える <br />\n",
    " - これまで通り訓練データに対する損失を小さくするように学習する <br />\n",
    " - エポックごとに未知のデータ（開発データ）に適用するとどの程度の損失になるか確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ・モデルの読み込み\n",
    "\n",
    " - データは極性判定コーパスの一部を利用 <br />\n",
    " - モデルは多層パーセプトロン（MLP）を利用 <br />\n",
    " - 文章をBoWで表現し，MLPで分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import MeCab\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from data import Sentence\n",
    "from data import DataSet\n",
    "from data import DataLoader\n",
    "from mlp import MultiLayer # 多層パーセプトロンを利用\n",
    "\n",
    "mecab_parser = MeCab.Tagger(\"-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "mecab_parser.parse(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    \"\"\"\n",
    "    データ読み込み用関数\n",
    "    Args:\n",
    "        path:対象ファイルのパス\n",
    "    Return:\n",
    "        tag_list (List[string])  :一行ごとのタグを要素に持つリスト\n",
    "        sentences_list (List[string]):一行ごとの文を要素に持つリスト\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        texts = f.readlines()\n",
    "\n",
    "    sentences_list,tag_list = [],[]\n",
    "    for row in texts:\n",
    "        sentence_list = []\n",
    "        tag,words = map(str,row.split(\"\\t\"))\n",
    "        tag_list.append(tag)\n",
    "        node = mecab_parser.parseToNode(words.replace('\\n',''))\n",
    "        while node:\n",
    "            analysis = node.feature.split(',')\n",
    "            if analysis[0] != 'BOS/EOS':\n",
    "                if analysis[0] == '名詞' and node.surface != '':\n",
    "                    sentence_list.append(node.surface)\n",
    "                elif analysis[6] != '':\n",
    "                    sentence_list.append(analysis[6])\n",
    "            node = node.next\n",
    "        sentences_list.append(sentence_list)\n",
    "    return tag_list, sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, devデータのパス\n",
    "train_path = \"data/train.txt\"\n",
    "dev_path = \"data/dev.txt\"\n",
    "\n",
    "# 文書の読み込み\n",
    "train_label, train = load_document(train_path)\n",
    "dev_label, dev = load_document(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練・開発データの読み込み\n",
    "train_data = DataSet(train, train_label)\n",
    "train_data_size = len(train_data)\n",
    "dev_data = DataSet(dev, dev_label, train_data.vocab)\n",
    "dev_data_size = len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW作成のために語彙辞書を出力\n",
    "output_file = open(\"vocab.txt\", \"w\")\n",
    "for v in train_data.vocab:\n",
    "    output_file.write(v + \"\\n\")\n",
    "\n",
    "# ラベル辞書を出力（後で）\n",
    "output_file = open(\"label_dict.tsv\", \"w\")\n",
    "for label_name, label_id in train_data.label_dict.items():\n",
    "    output_file.write(label_name +\"\\t\"+ str(label_id) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル\n",
    "multi_layer_model = MultiLayer(len(train_data.vocab), len(train_data.label_dict))\n",
    "\n",
    "# 損失の最小化のための最適化手法\n",
    "op = optim.SGD(multi_layer_model.parameters(), lr=0.1) # lr:learning rate (学習率)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習と評価\n",
    "訓練データはこれまで通りミニバッチ学習<br />\n",
    "エポックごとに開発データで損失を計算して，その時点でのモデルを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチサイズ\n",
    "minibatch_size = 10\n",
    "max_batch_no = train_data_size // minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "batch_no: 10/80 loss: 0.682607889175415\n",
      "batch_no: 20/80 loss: 0.6782971620559692\n",
      "batch_no: 30/80 loss: 0.6770572066307068\n",
      "batch_no: 40/80 loss: 0.7268162965774536\n",
      "batch_no: 50/80 loss: 0.6980679631233215\n",
      "batch_no: 60/80 loss: 0.6825952529907227\n",
      "batch_no: 70/80 loss: 0.6761987209320068\n",
      "batch_no: 80/80 loss: 0.6759682893753052\n",
      "epoch: 0  train_loss: 0.6571345955133439  dev_loss: 0.6751232892274857\n",
      "Epoch0 model saved\n",
      "==========\n",
      "epoch:  1\n",
      "batch_no: 10/80 loss: 0.6337900161743164\n",
      "batch_no: 20/80 loss: 0.5982596278190613\n",
      "batch_no: 30/80 loss: 0.6379952430725098\n",
      "batch_no: 40/80 loss: 0.607840359210968\n",
      "batch_no: 50/80 loss: 0.5912231206893921\n",
      "batch_no: 60/80 loss: 0.5052106380462646\n",
      "batch_no: 70/80 loss: 0.6442306041717529\n",
      "batch_no: 80/80 loss: 0.6346947550773621\n",
      "epoch: 1  train_loss: 0.5647016774863005  dev_loss: 0.6361295118927955\n",
      "Epoch1 model saved\n",
      "==========\n",
      "epoch:  2\n",
      "batch_no: 10/80 loss: 0.6207594871520996\n",
      "batch_no: 20/80 loss: 0.7911310195922852\n",
      "batch_no: 30/80 loss: 0.48791423439979553\n",
      "batch_no: 40/80 loss: 0.33329635858535767\n",
      "batch_no: 50/80 loss: 0.4469541609287262\n",
      "batch_no: 60/80 loss: 0.6425269246101379\n",
      "batch_no: 70/80 loss: 0.6689449548721313\n",
      "batch_no: 80/80 loss: 0.4338308274745941\n",
      "epoch: 2  train_loss: 0.4856404349207878  dev_loss: 0.6129384115338326\n",
      "Epoch2 model saved\n",
      "==========\n",
      "epoch:  3\n",
      "batch_no: 10/80 loss: 0.4274287819862366\n",
      "batch_no: 20/80 loss: 0.7097294330596924\n",
      "batch_no: 30/80 loss: 0.4835438132286072\n",
      "batch_no: 40/80 loss: 0.45242881774902344\n",
      "batch_no: 50/80 loss: 0.3457418978214264\n",
      "batch_no: 60/80 loss: 0.6138110160827637\n",
      "batch_no: 70/80 loss: 0.640326976776123\n",
      "batch_no: 80/80 loss: 0.5081708431243896\n",
      "epoch: 3  train_loss: 0.3980494886636734  dev_loss: 0.5862996205687523\n",
      "Epoch3 model saved\n",
      "==========\n",
      "epoch:  4\n",
      "batch_no: 10/80 loss: 0.2292066365480423\n",
      "batch_no: 20/80 loss: 0.3133821487426758\n",
      "batch_no: 30/80 loss: 0.5330098867416382\n",
      "batch_no: 40/80 loss: 0.4366229474544525\n",
      "batch_no: 50/80 loss: 0.2823107838630676\n",
      "batch_no: 60/80 loss: 0.1934160739183426\n",
      "batch_no: 70/80 loss: 0.27746689319610596\n",
      "batch_no: 80/80 loss: 0.5192578434944153\n",
      "epoch: 4  train_loss: 0.3249476937577128  dev_loss: 0.5843790955841541\n",
      "Epoch4 model saved\n",
      "==========\n",
      "epoch:  5\n",
      "batch_no: 10/80 loss: 0.29613396525382996\n",
      "batch_no: 20/80 loss: 0.23238353431224823\n",
      "batch_no: 30/80 loss: 0.2514054477214813\n",
      "batch_no: 40/80 loss: 0.4421427845954895\n",
      "batch_no: 50/80 loss: 0.4495980739593506\n",
      "batch_no: 60/80 loss: 0.26423531770706177\n",
      "batch_no: 70/80 loss: 0.4080783426761627\n",
      "batch_no: 80/80 loss: 0.2691195011138916\n",
      "epoch: 5  train_loss: 0.25100943092256783  dev_loss: 0.5910157404839993\n",
      "==========\n",
      "epoch:  6\n",
      "batch_no: 10/80 loss: 0.25400179624557495\n",
      "batch_no: 20/80 loss: 0.2471168041229248\n",
      "batch_no: 30/80 loss: 0.17175531387329102\n",
      "batch_no: 40/80 loss: 0.18745695054531097\n",
      "batch_no: 50/80 loss: 0.14573505520820618\n",
      "batch_no: 60/80 loss: 0.20421847701072693\n",
      "batch_no: 70/80 loss: 0.338411808013916\n",
      "batch_no: 80/80 loss: 0.19976815581321716\n",
      "epoch: 6  train_loss: 0.268707737326622  dev_loss: 0.7048458091914653\n",
      "==========\n",
      "epoch:  7\n",
      "batch_no: 10/80 loss: 0.1599392294883728\n",
      "batch_no: 20/80 loss: 0.1518426537513733\n",
      "batch_no: 30/80 loss: 0.20517583191394806\n",
      "batch_no: 40/80 loss: 0.16766934096813202\n",
      "batch_no: 50/80 loss: 0.42185157537460327\n",
      "batch_no: 60/80 loss: 0.16997073590755463\n",
      "batch_no: 70/80 loss: 0.2526780962944031\n",
      "batch_no: 80/80 loss: 0.3434608280658722\n",
      "epoch: 7  train_loss: 0.1828605005517602  dev_loss: 0.6049342349171638\n",
      "==========\n",
      "epoch:  8\n",
      "batch_no: 10/80 loss: 0.14812840521335602\n",
      "batch_no: 20/80 loss: 0.1417531967163086\n",
      "batch_no: 30/80 loss: 0.1413799226284027\n",
      "batch_no: 40/80 loss: 0.1412881761789322\n",
      "batch_no: 50/80 loss: 0.1334943175315857\n",
      "batch_no: 60/80 loss: 0.14770853519439697\n",
      "batch_no: 70/80 loss: 0.14443039894104004\n",
      "batch_no: 80/80 loss: 0.14663073420524597\n",
      "epoch: 8  train_loss: 0.17138101402670144  dev_loss: 0.6220678709447384\n",
      "==========\n",
      "epoch:  9\n",
      "batch_no: 10/80 loss: 0.1377316415309906\n",
      "batch_no: 20/80 loss: 0.1540118306875229\n",
      "batch_no: 30/80 loss: 0.14293022453784943\n",
      "batch_no: 40/80 loss: 0.13335585594177246\n",
      "batch_no: 50/80 loss: 0.16619016230106354\n",
      "batch_no: 60/80 loss: 0.12803901731967926\n",
      "batch_no: 70/80 loss: 0.14721767604351044\n",
      "batch_no: 80/80 loss: 0.33480173349380493\n",
      "epoch: 9  train_loss: 0.1552210906520486  dev_loss: 0.5886725947260857\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "min_dev_loss = 1000 # 開発データで良い評価（損失小）のモデルを保存したいので損失の最小値を得るための初期値を設定\n",
    "for epoch in range(10):\n",
    "\n",
    "    # 学習\n",
    "    print(\"epoch: \", epoch)\n",
    "\n",
    "    # モデルを学習モードに（パラメータの勾配計算や更新が可能）\n",
    "    multi_layer_model.train()\n",
    "\n",
    "    # ミニバッチ学習のためのデータローダー\n",
    "    train_data_loader = DataLoader(train_data, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    # ミニバッチごとに逆伝搬とパラメータ更新\n",
    "    for batch_no, (batch_input_vector, batch_target_values) in enumerate(train_data_loader):\n",
    "        # 順伝搬と損失計算\n",
    "        loss = multi_layer_model.forward_loss(batch_input_vector, batch_target_values)\n",
    "        if (batch_no + 1) % 10 == 0:\n",
    "            print(\"batch_no: {}/{} loss: {}\".format(batch_no+1, max_batch_no, loss))\n",
    "\n",
    "        # 逆伝搬 (勾配の設定)\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        op.step()\n",
    "\n",
    "        # 勾配の消去\n",
    "        multi_layer_model.zero_grad()\n",
    "\n",
    "\n",
    "    # epochごとに評価\n",
    "\n",
    "    # モデルを評価モードに（パラメータの勾配計算や更新が不可）\n",
    "    multi_layer_model.eval()\n",
    "\n",
    "    # trainデータのtotal lossを計算 (基本的に下がっていくはず)\n",
    "    train_data_loader = DataLoader(train_data, batch_size=minibatch_size, shuffle=False)\n",
    "    eval_train_loss = multi_layer_model.evaluate(train_data_loader)\n",
    "\n",
    "    # devデータのtotal lossを計算 (未知データなのでtrainデータよりは大きく，下がりづらいことが多い)\n",
    "    dev_data_loader = DataLoader(dev_data, batch_size=minibatch_size, shuffle=False)\n",
    "    eval_dev_loss = multi_layer_model.evaluate(dev_data_loader)\n",
    "\n",
    "    print(\"epoch: {}  train_loss: {}  dev_loss: {}\".format(epoch, eval_train_loss, eval_dev_loss))       \n",
    "\n",
    "    # 損失最小の場合モデルを保存\n",
    "    if eval_dev_loss < min_dev_loss:\n",
    "        print(\"Epoch{} model saved\".format(epoch))\n",
    "        torch.save(multi_layer_model.state_dict(), \"output_models/best_model\")\n",
    "        min_dev_loss = eval_dev_loss\n",
    "        \n",
    "    print(\"=\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
